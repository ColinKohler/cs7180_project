{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "sys.path.append('../')\n",
    "import dataset_loader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class And(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(And, self).__init__()\n",
    "        self.num_attention_maps = 2\n",
    "        \n",
    "    def forward(self, attention):\n",
    "        # Soft-logical and (Min)\n",
    "        return torch.min(attention, axis=2)\n",
    "    \n",
    "class Or(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Or, self).__init__()\n",
    "        self.num_attention_maps = 2\n",
    "        \n",
    "    def forward(self, attention):\n",
    "         # Soft-logical or (Max)\n",
    "        return torch.max(attention, axis=2)\n",
    "    \n",
    "class Id(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Id, self).__init__()\n",
    "        self.num_attention_maps = 1\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Find(nn.Module):\n",
    "    def __init__(self, input_size, num_kernels=64, kernel_size=5, find_what_dim=64):\n",
    "        super(Find, self).__init__()\n",
    "        self.num_attention_maps = 0\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.num_kernels = num_kernels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        # conv2(conv1(xvis), W*xtxt)\n",
    "        self.fc1 = nn.Linear(find_what_dim, (self.input_size[0] ** 2) * self.num_kernels)\n",
    "        self.conv1 = nn.Conv2d(self.input_size[-1], self.num_kernels, self.kernel_size)  \n",
    "        self.conv2 = nn.Conv2d(self.num_kernels, 1, self.kernel_size)\n",
    "        \n",
    "    # TODO: find_what is a bad name\n",
    "    def forward(self, context, find_what):\n",
    "        reshape = self.fc1(find_what).view(self.input_size[0], self.input_size[0], self.num_kernels)\n",
    "        conv_context = F.relu(self.conv1(context))\n",
    "        return F.relu(self.conv2(reshape  * conv_context))\n",
    "        \n",
    "class Relocate(nn.Module):\n",
    "    def __init__(self, input_size, num_kernels=64, kernel_size=5, relocate_where_dim=128):\n",
    "        super(Relocate, self).__init__()\n",
    "        self.num_attention_maps = 1\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.num_kernels = num_kernels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.relocate_where_dim = relocate_where_dim\n",
    "        \n",
    "        # conv2(conv1(xvis) * W1*sum(a * xvis) * W2*xtxt)\n",
    "        self.fc1 = nn.Linear(self.input_size[-1], (self.input_size[0] ** 2) * self.num_kernels)\n",
    "        self.fc2 = nn.Linear(self.relocate_where_dim, (self.input_size[0] ** 2) * self.num_kernels)\n",
    "        self.conv1 = nn.Conv2d(self.input_size[-1], self.num_kernels, self.kernel_size)  \n",
    "        self.conv2 = nn.Conv2d(self.num_kernels, 1, self.kernel_size)\n",
    "   \n",
    "    # TODO: relocate_where is a bad name\n",
    "    def forward(self, attention, context, relocate_where):\n",
    "        conv_xvis = F.relu(self.conv1(context))\n",
    "        xvis_attend = F.relu(self.fc1(torch.einsum('ijk,ijl->l', attention, context))) \n",
    "        W2_xtxt = F.relu(self.fc2(relocate_where))\n",
    "        return F.relu(self.conv2(conv_xvis * xvis_attend * W2_xtxt))        \n",
    "    \n",
    "class Exist(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Exist, self).__init__()\n",
    "        self.num_attention_maps = 1\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # W * vec(a)\n",
    "        self.fc1 = nn.Linear(self.input_size[-1]**2, 1)\n",
    "        \n",
    "    def forward(self, attention):\n",
    "        return self.fc1(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(QueryEncoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(self.input_size, self.hidden_size)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "    def resetHidden(self, batch_size):\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        self.hidden = (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
    "                       torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n",
    "        \n",
    "    def forward(self, query):\n",
    "        batch_size = query.size(0)\n",
    "        #import ipdb; ipdb.set_trace()\n",
    "        embeds = self.word_embeddings(query).view(1, batch_size, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "    \n",
    "        # TODO: Maybe reshape this if its bad\n",
    "        return lstm_out, self.hidden\n",
    "    \n",
    "class ContextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContextEncoder, self).__init__()\n",
    "        \n",
    "        # Init two conv layers to extract features (64 kernels)\n",
    "        self.conv1 = nn.Conv2d(3, 64, 10, stride=10)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 1, stride=1)\n",
    "        \n",
    "    def forward(self, context):\n",
    "        return F.relu(self.conv2(F.relu(self.conv1(context))))\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, M_dim, x_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.M_dim = M_dim\n",
    "        self.x_dim = x_dim\n",
    "        self.output_dim = M_dim[0] * M_dim[1] + x_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, self.output_dim)\n",
    "        \n",
    "    def forward(self):\n",
    "        # TODO: LSTMs have to have input but I dunno what it would be here\n",
    "        out, self.hidden = self.lstm(None, self.hidden)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        M_end = self.M_dim[0] * self.M_dim[1]\n",
    "        M = out[:M_end].view(self.M_dim[0], self.M_dim[1])\n",
    "        x = out[M_end:].view(1, -1)\n",
    "        \n",
    "        return M, x\n",
    "        \n",
    "    def resetHidden(self, batch_size):\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))\n",
    "    \n",
    "class MasterPolicy(nn.Module):\n",
    "    def __init__(self, attention_modules, anwser_modules, hidden_dim, context_size):\n",
    "        super(MasterPolicy, self).__init__()\n",
    "        self.attention_modules = attention_modules\n",
    "        self.anwser_modules = anwser_modules\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.context_size = context_size\n",
    "        \n",
    "        self.M_dim = (len(self.attention_modules), sum([m.num_attention_maps for m in self.attention_modules]))\n",
    "        self.x_dim = 64\n",
    "        self.decoder = Decoder(self.hidden_dim, self.M_dim, self.x_dim)\n",
    "        \n",
    "        a = torch.randn((self.context_size[0], self.context_size[1], self.M_dim[1]))\n",
    "    \n",
    "    def forward(self, query_hidden, context):\n",
    "        # TODO: Might have to do a more complex copy op\n",
    "        self.decoder.hidden[:] = query_hidden[:]\n",
    "        \n",
    "        # TODO: This for loop should be replaced with some sort of thresholding junk\n",
    "        for i in range(10):\n",
    "            self.M_t, self.x_t = decoder()\n",
    "            self.a_t, out = self.forward_1t(context)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def forward_1t(self, context):\n",
    "        b_t = torch.zeros(self.context_size, self.context_size, len(self.attention_modules))\n",
    "        \n",
    "        # Run all attention modules savi:ng output\n",
    "        num_att_map_inputs = np.array([module.num_attention_maps for module in self.attention_modules])\n",
    "        attention_map_input_index = np.cumsum(num_att_map_inputs.insert(0,0))\n",
    "        for i, module in enumerate(self.attention_modules):\n",
    "            # DONE - TODO: this 'i' is bad, needs to reflect the number of attention maps\n",
    "            attention = self.a_t[attention_map_input_index[i]:attention_map_input_index[i+1]]\n",
    "            if type(module) is Id:\n",
    "                b_t[:,:,i] = module.forward(attention)\n",
    "            elif type(module) is And:\n",
    "                b_t[:,:,i] = module.forward(attention)\n",
    "            elif type(module) is Or:\n",
    "                b_t[:,:,i] = module.forward(attention)\n",
    "            elif type(module) is Find:\n",
    "                b_t[:,:,i] = module.forward(context, self.x_t)\n",
    "            elif type(module) is Rellocate:\n",
    "                b_t[:,:,i] = module.forward(attention, context, self.x_t)\n",
    "            else:\n",
    "                raise ValueError('Invalid anwser Module: {}'.format(type(module)))\n",
    "            \n",
    "        # Run all anwser modules\n",
    "        for module in self.anwser_modules:\n",
    "            if type(module) is Exists:\n",
    "                out = module.forward(self.exist_attention)\n",
    "            else:\n",
    "                raise ValueError('Invalid anwser Module: {}'.format(type(module)))\n",
    "                \n",
    "        # Let N be the context size.  Then the 5 modules\n",
    "        # And, Or, Id, Find, Relocate\n",
    "        # output 5 attention maps, which stack to form an NxNx5 tensor\n",
    "        # called b.  Let M be 5x7 matrix of weights.  \n",
    "        # Set a = torch.einsum('ijk,kl->ijl',M,b)\n",
    "        # Then a is a NxNx7 tensor which gives the 7 NxNx1 input \n",
    "        # tensors for inputs to And, Or, Id, Relocate, Exist\n",
    "        return torch.einsum('ijk,kl->ijl', b_t, self.M_t), out\n",
    "\n",
    "class E2E_RNMN(nn.Module):\n",
    "    def __init__(self, query_size, hidden_size):\n",
    "        super(E2E_RNMN, self).__init__()\n",
    "        self.query_size = query_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.context_size = [7, 7, 64]\n",
    "        self.attention_modules = [And(), Or(), Id(), Find(self.context_size), Relocate(self.context_size)]\n",
    "        self.anwser_modules = [Exist(self.context_size)]\n",
    "        \n",
    "        self.query_encoder = QueryEncoder(self.query_size, self.hidden_size)\n",
    "        self.context_encoder = ContextEncoder()\n",
    "        self.master_policy = MasterPolicy(self.attention_modules, self.anwser_modules,\n",
    "                                          self.hidden_size, self.context_size)\n",
    "    \n",
    "    def forward(self, query, query_len, context):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Encode the query\n",
    "        self.query_encoder.resetHidden(batch_size)\n",
    "        encoder_outputs = torch.zeros(batch_size, 7, self.query_encoder.hidden_size, device=device)\n",
    "        max_query_len = query.size(1)\n",
    "        for ei in range(max_query_len):\n",
    "            encoder_output, encoder_hidden = self.query_encoder(query[:,ei])\n",
    "            encoder_outputs[:,ei,:] = encoder_output\n",
    "        \n",
    "        # Encode the context and start master policy forward pass'\n",
    "        encoded_context = self.context_encoder(context)\n",
    "        self.master_policy.decoder.resetHidden(batch_size)\n",
    "        return self.master_policy(self.query_encoder.hidden, encoded_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set hyperparams and load dataset\n",
    "lr = 1e-4\n",
    "hidden_size = 256\n",
    "batch_size = 64\n",
    "epochs = 1000\n",
    "\n",
    "query_lang, train_loader, test_loader = dataset_loader.createScalableShapesDataLoader('v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorToDevice(*tensors):\n",
    "    return [tensor.to(device) for tensor in tensors]\n",
    "\n",
    "def trainBatch(samples, queries, query_lens, labels):\n",
    "    # Transfer data to gpu/cpu and pass through model\n",
    "    samples, queries, query_lens, labels = tensorToDevice(samples, queries, query_lens, labels)\n",
    "    output = model(queries, query_lens, samples)\n",
    "    \n",
    "    # Compute loss & step optimzer\n",
    "    optimizer.zero_grad()\n",
    "    loss = F.nll_loss(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "    \n",
    "def testBatch(samples, queries, query_lens, labels):\n",
    "    with torch.no_grad():\n",
    "        # Transfer data to gpu/cpu and pass through model\n",
    "        samples, queries, query_lens, labels = tensorToDevice(samples, queries, query_lens, labels)\n",
    "        output = model(queries, query_lens, samples)\n",
    "        \n",
    "        # Compute loss & accuracy\n",
    "        loss = F.nll_loss(output, target)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct = pred.eq(labels.view_as(pred)).sum()\n",
    "    \n",
    "    return loss.item(), correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "Train Loss:0.0 | Train Acc:{0.0 | Test Loss:0.0 | Test Acc:0.0:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Decoder' object has no attribute 'num_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-189e95f935de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_lens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrainBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_lens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Test for a single epoch iterating over the minibatches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-8ba77a324b34>\u001b[0m in \u001b[0;36mtrainBatch\u001b[1;34m(samples, queries, query_lens, labels)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Transfer data to gpu/cpu and pass through model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_lens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensorToDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_lens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_lens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Compute loss & step optimzer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-32fd3f30cf72>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, query, query_len, context)\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;31m# Encode the context and start master policy forward pass'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mencoded_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaster_policy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresetHidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaster_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-32fd3f30cf72>\u001b[0m in \u001b[0;36mresetHidden\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mresetHidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# The axes semantics are (num_layers, minibatch_size, hidden_dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         return (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device),\n\u001b[0m\u001b[0;32m     63\u001b[0m                 torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    533\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 535\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Decoder' object has no attribute 'num_layers'"
     ]
    }
   ],
   "source": [
    "# Init model\n",
    "model = E2E_RNMN(query_lang.num_words, hidden_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Create TQDM progress bar\n",
    "pbar = tqdm.tqdm(total=epochs)\n",
    "pbar.set_description('Train Loss:0.0 | Train Acc:{0.0 | Test Loss:0.0 | Test Acc:0.0')\n",
    "\n",
    "train_losses, test_losses, test_accs = list(), list(), list()\n",
    "for epoch in range(epochs):\n",
    "    # Train for a single epoch iterating over the minibatches\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for samples, queries, query_lens, labels in train_loader:\n",
    "        train_loss += trainBatch(samples, queries, query_lens, labels)\n",
    "       \n",
    "    # Test for a single epoch iterating over the minibatches\n",
    "    model.eval()\n",
    "    test_loss, test_correct = 0, 0\n",
    "    for samples, queries, query_lens, labels in test_loader:\n",
    "        batch_loss, batch_correct = testBatch(samples, queries, query_lens, labels)\n",
    "        test_loss += batch_loss\n",
    "        test_correct += batch_correct\n",
    "    \n",
    "    # Bookkeeping\n",
    "    train_losses.appen(train_loss / len(train_loader.dataset))\n",
    "    test_losses.append(test_loss / len(test_loader.dataset))\n",
    "    test_accs.append(test_correct / len(test_loader.dataset))\n",
    "    \n",
    "    # Update progress bar\n",
    "    pbar.set_description('Train Loss:{:.3f} | Test Loss:{:.3f} | Test Acc:{:.3f}'.format(\n",
    "        train_losses[-1], test_losses[-1], test_accs[-1]))\n",
    "    pbar.update(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
